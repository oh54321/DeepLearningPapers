{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is an improvement of DQN which accounts for previous information from previous states\n",
        "# For example, if you trained the DQN algorithm to play Pong, it would not be able to predict which direction the ball moves in, since it is only trained on single instances of screens\n",
        "# So now, instead of training on single screens, we train on sequences of screens. Then we use an LSTM to carry information from previous states to the next state\n",
        "# To do this, we select random \"windows\" of playing a game, where we look at some random sequence of n states in the middle of the game for some n, and initialize the hidden state to be 0"
      ],
      "metadata": {
        "id": "PNC4oLShu9Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9iiqBE-FOk6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!wget http://www.atarimania.com/roms/Roms.rar \n",
        "!unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "!python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "!sudo pip install pyvirtualdisplay\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zu5C0KKGj6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "789ab2ff-a3e5-4bde-8815-aa04a365ec81",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "from pyvirtualdisplay import Display\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import random as r\n",
        "import copy\n",
        "from google.colab import drive\n",
        "import skimage\n",
        "from skimage import io as io2\n",
        "from skimage.transform import resize\n",
        "drive.mount('gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95niRA0hFThT"
      },
      "outputs": [],
      "source": [
        "# This code is not original, and taken from: https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
        "# This allows me to record videos of model playing Atari games in Google colab\n",
        "\n",
        "def query_environment(name):\n",
        "    env = gym.make(name)\n",
        "    spec = gym.spec(name)\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "    print(f\"Reward Range: {env.reward_range}\")\n",
        "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqnqw_zhNuaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cb1d05-27f6-451d-ac07-8f2ff4439d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 195.0\n"
          ]
        }
      ],
      "source": [
        "# Name is name of Atari game in Gym. Usually this is [name]-v0\n",
        "\n",
        "name = \"Breakout-v0\"\n",
        "query_environment(name)\n",
        "num_action = 4 # Number of possible actions in game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYr5MZQ8JlcZ"
      },
      "outputs": [],
      "source": [
        "# Converts array to tensor of floats\n",
        "def toTensor(arr):\n",
        "  return torch.tensor(arr, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KETJfV3fGebk"
      },
      "outputs": [],
      "source": [
        "# Value model is the model of our target nets and value nets. Here we use LSTMs to keep track of previous state information. \n",
        "#Thus, we also plug in hidden states and cell states to the model as well as the current game screen\n",
        "\n",
        "hidden_size = 256\n",
        "\n",
        "class valueModel(nn.Module):\n",
        "  def __init__(self, n_out, hidden_size):\n",
        "    super(valueModel, self).__init__()\n",
        "    self.value_net = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size = 8, stride = 4), \n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size = 4,  stride = 2),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 64, kernel_size = 3,  stride = 1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(1),\n",
        "    nn.Linear(3136, 512))\n",
        "    self.lstm = nn.LSTM(512,  hidden_size)\n",
        "    self.linear = nn.Sequential(nn.Flatten(), nn.Linear(hidden_size, n_out))\n",
        "  def forward(self, state, h, c):\n",
        "    output, (new_h, new_c) = self.lstm(self.value_net(state), (h, c))\n",
        "    return (self.linear(output), (new_h, new_c))\n",
        "\n",
        "value_net = valueModel(num_action, hidden_size)\n",
        "target_net = valueModel(num_action,  hidden_size)\n",
        "\n",
        "try:\n",
        "  value_net.load_state_dict(torch.load(\"gdrive/MyDrive/\"+str(name)+\"_bot_pompd\"))\n",
        "  target_net.load_state_dict(torch.load(\"gdrive/MyDrive/\"+str(name)+\"_bot_pompd\"))\n",
        "except:\n",
        "  print(\"Previous models not found\")\n",
        "\n",
        "optimizer = torch.optim.RMSprop(params = \n",
        "            value_net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnq7mKGwHZxY"
      },
      "outputs": [],
      "source": [
        " # Returns the action that gives the best possible predicted score based on the target network \n",
        "\n",
        "def chooseActionTarget(state, h, c):\n",
        "  vals, (_, _) = target_net(state, h, c)\n",
        "  vals = vals.detach().numpy()\n",
        "  return max([(vals[i], i) for i in range(len(vals))])\n",
        "\n",
        "# Returns the action that gives the best possible predicted score based on the value network \n",
        "\n",
        "def chooseAction(state, h, c):\n",
        "  vals, (h, c) = value_net(state, h, c)\n",
        "  vals = vals.detach().numpy()\n",
        "  return max([(vals[i], i) for i in range(len(vals))])\n",
        "\n",
        "# Sets the target network's parameters to be equal to the value network 's parameters\n",
        "\n",
        "def updateTarget():\n",
        "  target_net.load_state_dict(value_net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af6k5zckKd7S"
      },
      "outputs": [],
      "source": [
        "maxsize = 10000 # Maximum size of replay memory\n",
        "replay = []\n",
        "gamma = 1 # Size of gamma used in DQN\n",
        "window_size = 100 # Number of states in sequences we use to train our model\n",
        "loss_func = nn.HuberLoss() # Loss function used\n",
        "\n",
        "# Add a list of states, actions, rewards to our replay. Note here we're adding sequences of states, not individual states\n",
        "\n",
        "def addReplay(states, actions, rewards, dones):\n",
        "  if len(replay) == maxsize:\n",
        "    replay.pop(0)\n",
        "  for i in range(50):\n",
        "    left = r.randint(0, len(states)-1)\n",
        "    right = max(left+window_size, len(states))\n",
        "    replay.append((states[left:right], actions[left:right], rewards[left:right], dones[left:right]))\n",
        "\n",
        "# Gets one state at random from our replay memory\n",
        "\n",
        "def getOneReplay():\n",
        "  ret = replay[r.randint(0, len(replay)-1)]\n",
        "  return ret\n",
        "\n",
        "# Gets a sample of size n from replay memory\n",
        "\n",
        "def sampleReplay(n):\n",
        "  return [getOneReplay() for i in range(n)]\n",
        "\n",
        "def zero_if(x, b):\n",
        "  if b:\n",
        "    return 0\n",
        "  return x\n",
        "\n",
        "# Computes the loss among all sequences of states in a sample from replay memory\n",
        "\n",
        "def compute_loss(replay_sample):\n",
        "  loss = toTensor(0)\n",
        "  for game in replay_sample:\n",
        "    loss += compute_loss_game(game)\n",
        "  return loss\n",
        "\n",
        "# Computes the loss of an entire sequence of states, actions, and rewards, where hidden and cell states are kept track of over the game\n",
        "\n",
        "def compute_loss_game(game):\n",
        "  h = torch.zeros(hidden_size).unsqueeze(0)\n",
        "  c = torch.zeros(hidden_size).unsqueeze(0)\n",
        "  states = game[0]\n",
        "  actions = game[1]\n",
        "  rewards = game[2]\n",
        "  dones = game[3]\n",
        "  loss = toTensor(0)\n",
        "  for i in range(len(states)):\n",
        "    if i < len(states)-1:\n",
        "      loss += compute_loss_state(states[i], actions[i], states[i+1], rewards[i], dones[i], h, c)\n",
        "    elif dones[i]:\n",
        "      loss += compute_loss_state(states[i], actions[i], states[i], rewards[i], dones[i], h, c)\n",
        "  return loss\n",
        "\n",
        "# Computes the loss of a single state, given hidden states and cell states\n",
        "\n",
        "def compute_loss_state(state, action, observation, reward, done, h, c):\n",
        "  pred_val, (h, c) = value_net(toTensor(state).unsqueeze(0), h, c)\n",
        "  targ_max_val = chooseActionTarget(toTensor(observation).unsqueeze(0), h, c)[0]\n",
        "  actual_val = toTensor(reward + zero_if(gamma*targ_max_val, done))\n",
        "  loss = loss_func(pred_val,actual_val)\n",
        "  return loss\n",
        "\n",
        "# Trains our model on a batch of size bs\n",
        "\n",
        "def learnFromReplay(bs = 1024):\n",
        "  if len(replay) < 1000:\n",
        "    return 0\n",
        "  sample = sampleReplay(bs)\n",
        "  optimizer.zero_grad()\n",
        "  loss = compute_loss(sample)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return float(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snLEnvdAFYaF"
      },
      "outputs": [],
      "source": [
        "# Converts game screens to 84 x 84 black and white images\n",
        "\n",
        "def preproc(image):\n",
        "  return np.expand_dims(np.transpose(skimage.color.rgb2gray(resize(image, (84, 84)))), 0)\n",
        "\n",
        "# Plays the game once. Epsilon is the probability for a given state we choose an action randomly \"explore\" over \"exploit\", show described whether we want to return a video of us playing the game\n",
        "\n",
        "def playGame(show=False, epsilon = 0.1):\n",
        "  score = 0\n",
        "  losses = []\n",
        "\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  dones = []\n",
        "\n",
        "  if show:\n",
        "    env = wrap_env(gym.make(name))\n",
        "  else:\n",
        "    env = gym.make(name)\n",
        "  env._max_episode_steps = 1000\n",
        "  observation = env.reset()\n",
        "  observation = preproc(observation)\n",
        "  h = torch.zeros(hidden_size).unsqueeze(0)\n",
        "  c = torch.zeros(hidden_size).unsqueeze(0)\n",
        "  \n",
        "  while True:\n",
        "    env.render()\n",
        "    curr_state = observation\n",
        "\n",
        "\n",
        "    if not r.random() < epsilon:\n",
        "      _, action = chooseAction(toTensor(curr_state).unsqueeze(0), h, c)\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    action = int(action)\n",
        "\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "    observation = preproc(observation)\n",
        "    states.append(curr_state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    dones.append(done)\n",
        "    if done:\n",
        "        break\n",
        "  addReplay(states, actions, rewards, dones) # Adds the entire list of states and actions and rewards to replay, as we need multiple states in order to train our model to get better at preserving info of previous states\n",
        "  env.close()\n",
        "  if show:\n",
        "    show_video()\n",
        "  losses.append(learnFromReplay(32))\n",
        "  return (score, np.array(losses).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PJY1YB-arAs"
      },
      "outputs": [],
      "source": [
        "c = 10 # Number of games played before we update target\n",
        "count = 0\n",
        "scores = [] # Keeps track of scores and losses of games\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA3tW2LqQTzH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_games = 1000 # Plays this many games and trains our model\n",
        "for i in range(num_games):\n",
        "  score, loss = playGame(epsilon = 0.1)\n",
        "  scores.append(score)\n",
        "  losses.append(loss)\n",
        "  count += 1\n",
        "  if count % c == 0:\n",
        "    updateTarget()\n",
        "  plt.plot(scores)\n",
        "  plt.show()\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "  torch.save(value_net.state_dict(), \"gdrive/MyDrive/\"+str(name)+\"_bot_pompd\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DQN: Deep Recurrent Q-Learning for Partially Observable MDPs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}