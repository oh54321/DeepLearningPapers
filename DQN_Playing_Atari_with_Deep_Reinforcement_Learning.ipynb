{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is an implementation of Playing Atari with Deep Reinforcement Learning\n",
        "# The idea here is that our expected score V(s, a) for a game at a certain state s where we take action a is\n",
        "# V(s, a) = max_a' V(s', a')+r, where s' is the next state, r is the immediate score increment, and a' is taken from possible actions at the next state\n",
        "# In other words, we're assuming we always take actions to maximize our expected score\n",
        "# These are called the Bellman equations\n",
        "# The idea for the DQN algorithm is to view one of these sides as a prediction (the left) and the other as a target, then train V as a neural network\n",
        "# We predict the value of a game screen simply using a convolutional neural network\n",
        "# We use various stability tricks, such as using a replay memory to sample previous states randomly, so our training data is roughly uncorrelated"
      ],
      "metadata": {
        "id": "Y5_jahODX6kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psSAvvJEFAl0"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!wget http://www.atarimania.com/roms/Roms.rar \n",
        "!unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "!python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "!sudo pip install pyvirtualdisplay\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zu5C0KKGj6w",
        "outputId": "d27d1036-2742-44a7-d689-0bf09a151ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import random as r\n",
        "import copy\n",
        "#from google.colab import drive\n",
        "import skimage\n",
        "from skimage import io as io2\n",
        "from skimage.transform import resize\n",
        "drive.mount('gdrive')\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REPLAY_SIZE = 20000 # Maximum number of previous states saved in replay\n",
        "GAMMA = 1 # Parameter of DQN algorithm. We weight the reward of the nth time step in the future gamma^n * reward\n",
        "LOSS_FUNC = nn.HuberLoss() # Loss function used to measure difference between actual value (according to target net) and predicted value (according to value net)\n",
        "UPDATE_TARGET_LENGTH = 10 # How many games we want to play before updating the target"
      ],
      "metadata": {
        "id": "Na0dhOq0cY5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95niRA0hFThT"
      },
      "outputs": [],
      "source": [
        "# This code is not original, and taken from: https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
        "# This allows me to record videos of model playing Atari games in Google colab\n",
        "\n",
        "def query_environment(name):\n",
        "    env = gym.make(name)\n",
        "    spec = gym.spec(name)\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "    print(f\"Reward Range: {env.reward_range}\")\n",
        "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqnqw_zhNuaC",
        "outputId": "4f869038-ecbb-4e87-a436-c7b2b3ca93ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 195.0\n"
          ]
        }
      ],
      "source": [
        "# Name is name of Atari game in Gym. Usually this is [name]-v0\n",
        "\n",
        "name = \"Breakout-v0\"\n",
        "query_environment(name)\n",
        "num_action = 4 # Number of possible actions in game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KETJfV3fGebk",
        "outputId": "67e46484-e806-40bb-fdfc-628d82e91298"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# value_net is a convolutional neural network used to compute the expected score recieved given the current image displayed by a game\n",
        "\n",
        "value_net = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size = 8, stride = 4), \n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 32, kernel_size = 4,  stride = 2),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size = 3,  stride = 1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(1),\n",
        "    nn.Linear(3136, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_action)\n",
        ")\n",
        "\n",
        "# target_net is identical to value_net, but its parameters are frozen and it is only updated \n",
        "\n",
        "target_net = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size = 8, stride = 4), \n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 32, kernel_size = 4,  stride = 2),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size = 3,  stride = 1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(1),\n",
        "    nn.Linear(3136, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_action)\n",
        ")\n",
        "\n",
        "# This loads previously trained models\n",
        "\n",
        "try:\n",
        "  value_net.load_state_dict(torch.load(\"gdrive/MyDrive/\"+str(name)+\"_bot\"))\n",
        "  target_net.load_state_dict(torch.load(\"gdrive/MyDrive/\"+str(name)+\"_bot\"))\n",
        "except:\n",
        "  print(\"No previously trained model could be found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd-HvWxbNXEC"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params = \n",
        "            value_net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYr5MZQ8JlcZ"
      },
      "outputs": [],
      "source": [
        "# Function that easily converts any array to a tensor of floats\n",
        "\n",
        "def toTensor(arr, dtype = torch.float32):\n",
        "  return torch.tensor(arr, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnq7mKGwHZxY"
      },
      "outputs": [],
      "source": [
        " # Returns the action that gives the best possible predicted score based on the target network \n",
        "\n",
        "def chooseActionTargets(states):\n",
        "  with torch.no_grad():\n",
        "    vals = target_net(states).numpy()\n",
        "  return [max([(vals[k][i], i) for i in range(len(vals[k]))])[0] for k in range(len(vals))]\n",
        "\n",
        " # Returns the action that gives the best possible predicted score based on the value network \n",
        "\n",
        "def chooseAction(state):\n",
        "  with torch.no_grad():\n",
        "    vals = value_net(state)[0].numpy()\n",
        "  return max([(vals[i], i) for i in range(len(vals))])\n",
        "\n",
        " # Sets the target network's parameters to be equal to the value network 's parameters\n",
        "  \n",
        "def updateTarget():\n",
        "  target_net.load_state_dict(value_net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af6k5zckKd7S"
      },
      "outputs": [],
      "source": [
        "replay = []\n",
        "\n",
        "# Adds a state to the replay memory, along with the action taken, resulting state, reward given, and whether the game finished or not\n",
        "def addReplay(state, action, observation, reward, done):\n",
        "  if len(replay) == MAX_REPLAY_SIZE:\n",
        "    replay.pop(0)\n",
        "  replay.append((state, action, observation, reward, done))\n",
        "\n",
        "# Returns a single state from the replay memory. Right now, I've modified the function to give the final states of games more often, since these are more useful for determining the actual value of states\n",
        "def getOneReplay():\n",
        "  ret = replay[r.randint(0, len(replay)-1)]\n",
        "  while ret[4] == False:\n",
        "   ret = replay[r.randint(0, len(replay)-1)]\n",
        "   if r.random() >= 0.9:\n",
        "      return ret\n",
        "  return ret\n",
        "\n",
        "# Returns a sample of size n of previous states in replay memory\n",
        "def sampleReplay(n):\n",
        "  return [getOneReplay() for i in range(n)]\n",
        "\n",
        "# Returns 0 if b, else x. Useful for finding target rewards in the case when a game is finished\n",
        "def zero_if(x, b):\n",
        "  if b:\n",
        "    return 0\n",
        "  return x\n",
        "\n",
        "# Computers the loss of a replay sample\n",
        "def compute_loss(replay_sample):\n",
        "  states = toTensor([tup[0] for tup in replay_sample]) \n",
        "  actions = toTensor([tup[1] for tup in replay_sample], dtype = torch.int64) \n",
        "  observations = toTensor([tup[2] for tup in replay_sample])\n",
        "  rewards = toTensor([tup[3] for tup in replay_sample])\n",
        "  dones = [tup[4] for tup in replay_sample]\n",
        "  pred_vals = torch.squeeze(value_net(states).gather(1, actions.unsqueeze(1))) # Predicted values of states based on value net\n",
        "  targ_max_vals = chooseActionTargets(observations) # Best possible actions of new states based on target network, used to find target values\n",
        "  actual_vals = rewards + toTensor([zero_if(GAMMA*targ_max_vals[i], int(dones[i]) == 1) for i in range(len(replay_sample))]) # Actual values are gamma*max target value + reward, or just the reward if it's a terminating state, according to DQN\n",
        "  loss = LOSS_FUNC(pred_vals, actual_vals) \n",
        "  return loss\n",
        "\n",
        "# Trains value net on a replay batch of size bs\n",
        "def learnFromReplay(bs = 1024):\n",
        "  if len(replay) < 1000: # If the replay is too small, don't bother training. \n",
        "    return 0\n",
        "  optimizer.zero_grad()\n",
        "  sample = sampleReplay(bs)\n",
        "  loss = compute_loss(sample)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return float(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snLEnvdAFYaF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Used to preprocess an image given by the game. Right now, we convert to an 84 x 84 black and white image\n",
        "def preproc(image):\n",
        "  return np.expand_dims(np.transpose(skimage.color.rgb2gray(resize(image, (84, 84)))), 0)\n",
        "\n",
        "# Plays the game once. Epsilon is the probability for a given state we choose an action randomly \"explore\" over \"exploit\", show described whether we want to return a video of us playing the game\n",
        "def playGame(show=False, epsilon = 0.1):\n",
        "\n",
        "  score = 0\n",
        "  losses = []\n",
        "\n",
        "  if show:\n",
        "    env = wrap_env(gym.make(name).env)\n",
        "  else:\n",
        "    env = gym.make(name).env\n",
        "  observation = env.reset()\n",
        "  observation = preproc(observation)\n",
        "\n",
        "  while True:\n",
        "    env.render()\n",
        "    curr_state = observation # Current state\n",
        "    if not r.random() < epsilon:\n",
        "      _, action = chooseAction(toTensor(curr_state).unsqueeze(0)) # Chooses an action\n",
        "    else:\n",
        "      action = env.action_space.sample() # Unless, with probability epsilon, act randomly\n",
        "    action = int(action)\n",
        "    \n",
        "    observation, reward, done, info = env.step(action) # Get reward and new state\n",
        "    score += reward # Update total score\n",
        "    observation = preproc(observation)\n",
        "    addReplay(curr_state, action, observation, reward, done) # Add current state to replay memory\n",
        "    if done:\n",
        "        break\n",
        "  env.close()\n",
        "  if show:\n",
        "    show_video()\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PJY1YB-arAs"
      },
      "outputs": [],
      "source": [
        "scores = [] # Used to keep track of scores of games and training losses, to measure performance over time\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xA3tW2LqQTzH"
      },
      "outputs": [],
      "source": [
        "num_games = 0\n",
        "avg_num = 100 # Number of batches we train the model for after playing one game\n",
        "train_iter = 1000\n",
        "while num_games < train_iter:\n",
        "  score = playGame(False, epsilon = 0.1) \n",
        "  avg = 0\n",
        "  for i in range(avg_num):\n",
        "    avg += learnFromReplay(32)\n",
        "  loss = avg/avg_num\n",
        "  scores.append(score)\n",
        "  losses.append(loss)\n",
        "  num_games  += 1\n",
        "  if num_games % UPDATE_TARGET_LENGTH == 0:\n",
        "    updateTarget()\n",
        "plt.plot(scores) # Plots scores and losses over time\n",
        "plt.show()\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "torch.save(value_net.state_dict(), \"gdrive/MyDrive/\"+str(name)+\"_bot\") # Saves model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIXf-ksE0qRC"
      },
      "outputs": [],
      "source": [
        "playGame(True, epsilon = 0.00) # Now plays the game with no randomness, and shows video of performance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DQN: Playing Atari with Deep Reinforcement Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}